{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision.io import read_video\n",
    "from torchvision.utils import save_image\n",
    "from typing import List, OrderedDict, Tuple\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision import transforms\n",
    "import torchinfo\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from enum import Enum\n",
    "import xml.etree.ElementTree as ET \n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frcnn_model():\n",
    "  frcnn = fasterrcnn_resnet50_fpn_v2()\n",
    "  frcnn = frcnn.eval().cuda()\n",
    "  torchinfo.summary(frcnn)\n",
    "  return frcnn\n",
    "\n",
    "class ObjectType(Enum):\n",
    "  CONSTRUCTION = 1\n",
    "  CYCLIST = 2\n",
    "  VEHICLE = 3\n",
    "  PEDESTRIAN = 4\n",
    "  BUS = 5\n",
    "\n",
    "class DetectedObject:\n",
    "  def __init__(self, xmin, ymin, xmax, ymax):\n",
    "    self.xmin = xmin\n",
    "    self.ymin = ymin\n",
    "    self.xmax = xmax\n",
    "    self.ymax = ymax\n",
    "  \n",
    "  def __str__(self):\n",
    "    return f'({self.xmin}, {self.ymin}, {self.xmax}, {self.ymax})'\n",
    "    \n",
    "class AnnotationObject:\n",
    "  def __init__(self, object: ET.Element):\n",
    "    self.type: ObjectType = ObjectType.__dict__[object.find(\"./name\").text.upper()]\n",
    "    self.xmin = float(object.find(\"./bndbox/xmin\").text)\n",
    "    self.xmax = float(object.find(\"./bndbox/xmax\").text)\n",
    "    self.ymin = float(object.find(\"./bndbox/ymin\").text)\n",
    "    self.ymax = float(object.find(\"./bndbox/ymax\").text)\n",
    "    \n",
    "  def __str__(self):\n",
    "    return f'({self.xmin}, {self.ymin}, {self.xmax}, {self.ymax})'\n",
    "  \n",
    "  @property\n",
    "  def area(self):\n",
    "    return (self.ymax-self.ymin)*(self.xmax-self.xmin)\n",
    "    \n",
    "  def intersect_area(self, other):\n",
    "    dx = min(other.xmax, self.xmax) - max(other.xmin, self.xmin)\n",
    "    dy = min(other.ymax, self.ymax) - max(other.ymin, self.ymin)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "      return dx*dy\n",
    "  \n",
    "  def difference(self, other):\n",
    "    intersect = self.intersect_area(other)\n",
    "    if intersect is None:\n",
    "      return 1\n",
    "    my_area = self.area\n",
    "    other_area = (other.ymax-other.ymin)*(other.xmax-other.xmin)\n",
    "    score = ((my_area+other_area) - (2*intersect))/my_area\n",
    "    return score if score<1 else 1\n",
    "\n",
    "class Annotation:\n",
    "  def __init__(self, tree: ET.ElementTree):\n",
    "    root = tree.getroot()\n",
    "    self.image_filename = root.find(\"filename\").text\n",
    "    self.objects: List[AnnotationObject] = []\n",
    "    self.types = set()\n",
    "    for object in root.findall(\"./object\"):\n",
    "      anot = AnnotationObject(object)\n",
    "      self.objects.append(anot)\n",
    "      self.types.add(anot.type)\n",
    "    \n",
    "\n",
    "def load_canada_dataset():\n",
    "  parent_dir = Path(\"canada-dataset\")\n",
    "  annotation_dir = parent_dir.joinpath(\"Annotations\")\n",
    "  image_dir = parent_dir.joinpath(\"JPEGImages\")\n",
    "  \n",
    "  annotations: Dict[str,Annotation] = {}\n",
    "  for annotation in annotation_dir.glob(\"*.xml\"):\n",
    "    tree = ET.parse(annotation)\n",
    "    a = Annotation(tree)\n",
    "    annotations[a.image_filename] = a\n",
    "  \n",
    "  images: Dict[str,Image.Image] = {}\n",
    "  for img_file in image_dir.glob(\"*.jpeg\"):\n",
    "    images[img_file.name] = Image.open(img_file)\n",
    "    images[img_file.name].load()\n",
    "  \n",
    "  return annotations, images\n",
    "\n",
    "def show_rectangle(img: Image.Image, xmin, ymin, xmax, ymax):\n",
    "  tmp_img = img.copy()\n",
    "  draw = ImageDraw.Draw(tmp_img)\n",
    "  draw.rectangle([(xmin,ymin),(xmax,ymax)], outline=\"red\", width=1)\n",
    "  tmp_img.show()\n",
    "  print(tmp_img.size)\n",
    "  print(xmin, ymin, xmax, ymax)\n",
    "\n",
    "def show_annotations(img: Image.Image, annotation: Annotation):\n",
    "  tmp_img = img.copy()\n",
    "  draw = ImageDraw.Draw(tmp_img)\n",
    "  for obj in annotation.objects:\n",
    "    print((obj.xmin,obj.ymin),(obj.xmax,obj.ymax))\n",
    "    draw.rectangle([(obj.xmin,obj.ymin),(obj.xmax,obj.ymax)], outline=\"red\", width=1)\n",
    "  tmp_img.show()\n",
    "\n",
    "def apply_annotations(\n",
    "  img: Image.Image, true_annotations: List[AnnotationObject], \n",
    "  predicted_annotations, other_annotations, dif_scores\n",
    "):\n",
    "  tmp_img = img.copy()\n",
    "  draw = ImageDraw.Draw(tmp_img)\n",
    "  for i, true_annot in enumerate(true_annotations):\n",
    "    draw.rectangle([(true_annot.xmin,true_annot.ymin),(true_annot.xmax,true_annot.ymax)], outline=\"green\", width=1)\n",
    "  for p_annot in predicted_annotations:\n",
    "    draw.rectangle([(p_annot.xmin,p_annot.ymin),(p_annot.xmax,p_annot.ymax)], outline=\"orange\", width=1)\n",
    "  for p_annot in other_annotations:\n",
    "    draw.rectangle([(p_annot.xmin,p_annot.ymin),(p_annot.xmax,p_annot.ymax)], outline=\"blue\", width=1)\n",
    "  for i, true_annot in enumerate(true_annotations):\n",
    "    draw.text((true_annot.xmin,true_annot.ymin), f'{dif_scores[i]:.2f}', \"red\", font=ImageFont.truetype(\"arial.ttf\", 15))\n",
    "  return tmp_img\n",
    "\n",
    "def extract_frcnn_detections(pred_bboxes):\n",
    "  detections = []\n",
    "  for box in pred_bboxes:\n",
    "    detections.append(DetectedObject(int(box[1]), int(box[0]), int(box[3]), int(box[2])))\n",
    "  return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations, images = load_canada_dataset()\n",
    "frcnn = load_frcnn_model().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "frcnn = frcnn.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 800, 977])\n",
      "(977, 800)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img_raw = images['000001.jpeg']\n",
    "img_raw = img_raw.resize((int(img_raw.size[0]*(800/img_raw.size[1])),800))\n",
    "img = transform(img_raw).unsqueeze(0).cuda()\n",
    "print(img.shape)\n",
    "print(img_raw.size)\n",
    "outputs = frcnn(img)\n",
    "pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "labels = outputs[0]['labels']\n",
    "boxes = extract_frcnn_detections(pred_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(12, 70, 37, 105)',\n",
       " '(0, 45, 23, 71)',\n",
       " '(0, 39, 18, 58)',\n",
       " '(16, 51, 45, 95)',\n",
       " '(6, 33, 20, 54)',\n",
       " '(7, 33, 36, 59)',\n",
       " '(19, 13, 61, 50)',\n",
       " '(7, 26, 27, 53)',\n",
       " '(23, 27, 43, 57)',\n",
       " '(13, 51, 31, 89)',\n",
       " '(19, 83, 38, 119)',\n",
       " '(10, 41, 32, 76)',\n",
       " '(18, 61, 34, 92)',\n",
       " '(7, 66, 23, 91)',\n",
       " '(16, 38, 42, 58)',\n",
       " '(24, 77, 48, 121)',\n",
       " '(4, 38, 29, 63)',\n",
       " '(11, 74, 30, 97)',\n",
       " '(38, 58, 53, 86)',\n",
       " '(40, 53, 54, 74)',\n",
       " '(4, 30, 7, 68)',\n",
       " '(22, 64, 41, 100)',\n",
       " '(10, 61, 27, 86)',\n",
       " '(10, 41, 23, 64)',\n",
       " '(45, 87, 54, 136)',\n",
       " '(8, 30, 11, 62)',\n",
       " '(49, 18, 68, 38)',\n",
       " '(0, 43, 4, 75)',\n",
       " '(41, 69, 57, 94)',\n",
       " '(7, 36, 11, 65)',\n",
       " '(9, 22, 14, 61)',\n",
       " '(39, 40, 56, 59)',\n",
       " '(12, 40, 15, 80)',\n",
       " '(6, 40, 9, 74)',\n",
       " '(29, 84, 38, 128)',\n",
       " '(4, 58, 8, 91)',\n",
       " '(7, 65, 11, 94)',\n",
       " '(1, 52, 5, 85)',\n",
       " '(6, 51, 17, 75)',\n",
       " '(8, 72, 12, 101)',\n",
       " '(45, 82, 57, 108)',\n",
       " '(9, 10, 17, 64)',\n",
       " '(49, 85, 60, 114)',\n",
       " '(7, 76, 13, 110)',\n",
       " '(0, 58, 16, 72)',\n",
       " '(6, 66, 10, 102)',\n",
       " '(10, 60, 15, 116)',\n",
       " '(7, 79, 22, 94)',\n",
       " '(14, 86, 28, 105)',\n",
       " '(30, 87, 42, 119)',\n",
       " '(6, 82, 10, 116)',\n",
       " '(9, 95, 26, 105)',\n",
       " '(15, 66, 24, 99)',\n",
       " '(14, 52, 27, 76)',\n",
       " '(5, 113, 12, 164)',\n",
       " '(1, 103, 6, 143)',\n",
       " '(9, 86, 23, 99)',\n",
       " '(7, 97, 12, 143)',\n",
       " '(4, 94, 8, 134)',\n",
       " '(7, 81, 11, 118)',\n",
       " '(6, 126, 13, 153)',\n",
       " '(33, 50, 49, 75)',\n",
       " '(11, 74, 16, 125)',\n",
       " '(8, 117, 14, 145)',\n",
       " '(5, 142, 10, 203)',\n",
       " '(4, 154, 8, 217)',\n",
       " '(9, 108, 14, 137)',\n",
       " '(11, 117, 17, 155)',\n",
       " '(6, 109, 11, 135)',\n",
       " '(11, 98, 17, 135)',\n",
       " '(9, 165, 15, 218)',\n",
       " '(5, 127, 10, 156)',\n",
       " '(4, 133, 10, 181)',\n",
       " '(31, 54, 47, 87)',\n",
       " '(7, 136, 15, 181)',\n",
       " '(9, 85, 14, 116)',\n",
       " '(0, 38, 4, 148)',\n",
       " '(0, 0, 0, 123)',\n",
       " '(0, 5, 35, 69)',\n",
       " '(0, 9, 16, 88)',\n",
       " '(0, 0, 13, 53)',\n",
       " '(0, 0, 4, 108)',\n",
       " '(0, 0, 1, 29)',\n",
       " '(0, 44, 1, 121)',\n",
       " '(0, 0, 24, 31)',\n",
       " '(0, 0, 8, 127)',\n",
       " '(18, 64, 36, 140)',\n",
       " '(23, 31, 49, 46)',\n",
       " '(20, 83, 43, 151)',\n",
       " '(10, 50, 43, 71)',\n",
       " '(2, 19, 51, 58)',\n",
       " '(23, 80, 34, 144)',\n",
       " '(17, 115, 42, 152)',\n",
       " '(24, 41, 45, 55)',\n",
       " '(10, 35, 19, 76)',\n",
       " '(2, 41, 13, 63)',\n",
       " '(4, 41, 18, 92)',\n",
       " '(3, 48, 8, 80)',\n",
       " '(12, 115, 31, 166)',\n",
       " '(47, 95, 57, 138)']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(box) for box in boxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "def test_frcnn(out_dir: str, model):\n",
    "  out_dir = Path(out_dir)\n",
    "  out_dir.mkdir(exist_ok=True)\n",
    "  track_list = set([ObjectType.BUS.name, ObjectType.VEHICLE.name, ObjectType.PEDESTRIAN.name, ObjectType.CYCLIST.name])\n",
    "  detected_classes = set()\n",
    "  summary = {}\n",
    "  for tot in track_list:\n",
    "    summary[f'total_{tot}_misses'] = 0\n",
    "    summary[f'total_{tot}_loss_sum'] = 0\n",
    "    summary[f'total_{tot}_objects'] = 0\n",
    "  summary[f'total_VEHICLE_detections'] = 0\n",
    "  summary[f'total_objects'] = 0\n",
    "  summary[f'total_PERSON_detections'] = 0\n",
    "      \n",
    "  for img_file, annotation in annotations.items():\n",
    "    img_raw = images[img_file]\n",
    "    img = transform(img_raw).unsqueeze(0).cpu()\n",
    "    out = model(img)[0]['boxes'].detach().cpu().numpy()\n",
    "    detected_objects = extract_frcnn_detections(out)\n",
    "    \n",
    "    detected_interests = []\n",
    "    detected_car_count = 0\n",
    "    detected_person_count = 0\n",
    "    detected_other = []\n",
    "    for index, object in enumerate(detected_objects):\n",
    "      # Collect all detected objects for image annotations\n",
    "      detected_car_count += 1\n",
    "      summary[f'total_VEHICLE_detections'] += 1\n",
    "      detected_interests.append(object)\n",
    "      \n",
    "    dif_scores = []\n",
    "    score_sum = 0\n",
    "    miss_count = 0\n",
    "    true_annotations = []\n",
    "    summary['total_objects'] += len(annotation.objects)\n",
    "    for true_obj in annotation.objects:\n",
    "      tot = true_obj.type.name\n",
    "      if tot in track_list:\n",
    "        true_annotations.append(true_obj)\n",
    "        summary[f'total_{tot}_objects'] += 1\n",
    "        best = 1\n",
    "        for object in detected_interests:\n",
    "          dif = true_obj.difference(object)\n",
    "          if dif is not None:\n",
    "            if dif < best:\n",
    "              best = dif\n",
    "        dif_scores.append(best)\n",
    "        score_sum += best\n",
    "        summary[f'total_{tot}_loss_sum'] += best\n",
    "        if best == 1:\n",
    "          miss_count += 1\n",
    "          summary[f'total_{tot}_misses'] += 1\n",
    "    \n",
    "    avg_score = score_sum/len(dif_scores) if len(dif_scores)>0 else 0\n",
    "    annotated = apply_annotations(img_raw, true_annotations, detected_interests, detected_other, dif_scores)\n",
    "    annotated.save(out_dir.joinpath(f'{img_file.rstrip(\".jpeg\")}-{avg_score:.2f}-{miss_count}-{len(dif_scores)}.jpeg'))\n",
    "    \n",
    "  total_score_sum = 0\n",
    "  total_objects = 0\n",
    "  total_found = 0\n",
    "  for tot in track_list:\n",
    "    score = summary[f'total_{tot}_loss_sum']\n",
    "    objects = summary[f'total_{tot}_objects']\n",
    "    misses = summary[f'total_{tot}_misses']\n",
    "    summary[f'total_{tot}_avg_loss'] = score/objects if objects > 0 else 0\n",
    "    summary[f'total_{tot}_pct_found'] = (objects-misses)/objects if objects > 0 else 1\n",
    "    total_found += (objects-misses)\n",
    "    total_score_sum += score\n",
    "    total_objects += objects\n",
    "  summary[f'total_avg_loss'] = total_score_sum/total_objects\n",
    "  summary[f'total_pct_found'] = total_found/total_objects\n",
    "  summary[f'detected_classes'] = list(detected_classes)\n",
    "  with open(out_dir.joinpath('summary.json'), 'w') as outfile:\n",
    "    json.dump(summary, outfile)\n",
    "    \n",
    "#test_yolo('test-small')\n",
    "test_frcnn('test-frcnn', frcnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
